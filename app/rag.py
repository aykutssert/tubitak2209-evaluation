from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
from typing import Dict, List, Any, Tuple
from openai import OpenAI
from dotenv import load_dotenv
import re
load_dotenv()

class TurkishRAGSystem:
    def __init__(self):
        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ model
        self.model = None
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        
        # Section keywords for smart matching - Enhanced with rubric knowledge
        self.section_keywords = {
            'genel_bilgiler': ['baÅŸlÄ±k', 'baÅŸvuru', 'danÄ±ÅŸman', 'kurum', 'isim', 'ad', 'araÅŸtÄ±rmacÄ±', 'Ã§alÄ±ÅŸan', 'sahibi'],
            'amaÃ§_hedef': ['amaÃ§', 'hedef', 'niÃ§in', 'neden', 'amacÄ±', 'maksadÄ±', 'Ã¶lÃ§Ã¼lebilir', 'gerÃ§ekÃ§i'],
            'Ã¶nem_deÄŸer': ['Ã¶nem', 'Ã¶zgÃ¼n', 'deÄŸer', 'hipotez', 'literatÃ¼r', 'katkÄ±', 'eksiklik', 'sorun'],
            'yÃ¶ntem': ['yÃ¶ntem', 'teknik', 'nasÄ±l', 'sÃ¼reÃ§', 'araÅŸtÄ±rma', 'metod', 'tasarÄ±m', 'istatistik'],
            'bÃ¼tÃ§e': ['bÃ¼tÃ§e', 'maliyet', 'para', 'tl', 'fiyat', 'kalem', 'harcama', 'talep', 'tutarlÄ±'],
            'zaman': ['zaman', 'takvim', 'sÃ¼re', 'Ã§izelge', 'iÅŸ paketi', 'program', 'baÅŸarÄ±', 'Ã¶lÃ§Ã¼t'],
            'risk': ['risk', 'zorluk', 'problem', 'sorun', 'b planÄ±', 'gÃ¼Ã§lÃ¼k', 'yÃ¶netim'],
            'etki': ['etki', 'sonuÃ§', 'Ã§Ä±ktÄ±', 'fayda', 'yaygÄ±n', 'deÄŸer', 'bilimsel', 'akademik', 'ticari'],
            'Ã¶zet': ['Ã¶zet', 'summary', 'kÄ±saca', 'Ã¶zetle'],
            'araÅŸtÄ±rma_olanaklarÄ±': ['altyapÄ±', 'laboratuvar', 'ekipman', 'olanak', 'makine', 'teÃ§hizat']
        }
        
        # Rubric-based section priority mapping
        self.rubric_section_map = {
            'genel_bilgiler': {
                'priority': 10,
                'keywords': ['baÅŸvuru sahibi', 'danÄ±ÅŸman', 'kurum', 'baÅŸlÄ±k', 'isim', 'ad']
            },
            'Ã¶zet': {
                'priority': 9,
                'keywords': ['Ã¶zgÃ¼n deÄŸer', 'yÃ¶ntem', 'yÃ¶netim', 'yaygÄ±n etki']
            },
            'amaÃ§_hedef': {
                'priority': 9,
                'keywords': ['amaÃ§', 'hedef', 'Ã¶lÃ§Ã¼lebilir', 'gerÃ§ekÃ§i', 'ulaÅŸÄ±labilir']
            },
            'Ã¶nem_deÄŸer': {
                'priority': 8,
                'keywords': ['Ã¶zgÃ¼n deÄŸer', 'literatÃ¼r', 'eksiklik', 'hipotez', 'atÄ±f']
            },
            'yÃ¶ntem': {
                'priority': 7,
                'keywords': ['yÃ¶ntem', 'teknik', 'araÅŸtÄ±rma tasarÄ±m', 'istatistik', 'fizibilite']
            },
            'bÃ¼tÃ§e': {
                'priority': 8,
                'keywords': ['bÃ¼tÃ§e kalem', 'tutarlÄ±', 'ihtiyaÃ§', 'toplam bÃ¼tÃ§e']
            },
            'iÅŸ_zaman': {
                'priority': 6,
                'keywords': ['iÅŸ paketi', 'baÅŸarÄ± Ã¶lÃ§Ã¼t', 'izlenebilir', 'literatÃ¼r tarama']
            },
            'risk': {
                'priority': 5,
                'keywords': ['risk', 'b planÄ±', 'yÃ¶netim']
            },
            'araÅŸtÄ±rma_olanaklarÄ±': {
                'priority': 6,
                'keywords': ['altyapÄ±', 'laboratuvar', 'ekipman']
            },
            'yaygÄ±n_etki': {
                'priority': 7,
                'keywords': ['bilimsel etki', 'akademik', 'ekonomik', 'sosyal', 'ticari']
            }
        }

    def load_model(self):
        if self.model is None:
            try:
                print("ðŸ“¥ Loading Turkish embedding model...")
                self.model = SentenceTransformer("emrecan/bert-base-turkish-cased-mean-nli-stsb-tr")
                print("âœ… Model loaded successfully!")
                
                # Test model
                test_embedding = self.model.encode(["test"], convert_to_numpy=True)
                print(f"ðŸ§ª Model test successful - embedding shape: {test_embedding.shape}")
                
            except Exception as e:
                print(f"âŒ Model loading error: {e}")
                print("ðŸ”„ Continuing with keyword-only search...")
                self.model = None

    def preprocess_user_query(self, original_query: str) -> Dict[str, str]:
        """
        KullanÄ±cÄ±nÄ±n orijinal prompt'unu RAG iÃ§in optimize edilmiÅŸ forma Ã§evirir
        """
        preprocessing_prompt = f"""Sen bir RAG sistemi uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n sorusunu analiz et ve SADECE JSON dÃ¶ndÃ¼r.

Soru: "{original_query}"

JSON formatÄ± (tÄ±rnak iÅŸaretlerine dikkat et):

{{
  "optimized_query": "anahtar kelimeler",
  "search_intent": "analytical", 
  "key_concepts": ["kavram1", "kavram2"],
  "document_sections": ["bÃ¶lÃ¼m1", "bÃ¶lÃ¼m2"],
  "response_style": "detailed",
  "complexity_level": "medium",
  "is_multi_intent": true,
  "sub_queries": [
    {{"intent": "aÃ§Ä±klama", "keywords": ["proje", "aÃ§Ä±klama"], "sections": ["Ã¶zet"]}},
    {{"intent": "bÃ¼tÃ§e", "keywords": ["bÃ¼tÃ§e", "maliyet"], "sections": ["bÃ¼tÃ§e"]}}
  ]
}}

Kurallar:
- search_intent: specific, broad, comparative, analytical, multi_intent
- response_style: detailed, summary, list, analytical  
- complexity_level: low, medium, high
- is_multi_intent: EÄŸer soru birden fazla farklÄ± konuyu soruyorsa true
- sub_queries: Her alt soru iÃ§in ayrÄ± analiz (max 5 alt soru)
- SADECE JSON dÃ¶ndÃ¼r, baÅŸka hiÃ§bir ÅŸey yazma
- TÄ±rnak iÅŸaretlerini doÄŸru kullan"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Sen JSON formatÄ± uzmanÄ±sÄ±n. Sadece geÃ§erli JSON dÃ¶ndÃ¼rÃ¼rsÃ¼n. TÄ±rnak iÅŸaretlerine dikkat edersin."},
                    {"role": "user", "content": preprocessing_prompt}
                ],
                max_tokens=300,
                temperature=0.0
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSON'u temizle
            if response_text.startswith('```json'):
                response_text = response_text.replace('```json', '').replace('```', '').strip()
            elif response_text.startswith('```'):
                response_text = response_text.replace('```', '').strip()
            
            # JSON'da problematik karakterleri temizle
            response_text = response_text.replace('\n', ' ').replace('\r', ' ')
            
            # Parse JSON
            result = json.loads(response_text)
            
            # Sonucu doÄŸrula ve varsayÄ±lanlar ekle
            processed_query = {
                "original_query": original_query,
                "optimized_query": result.get("optimized_query", original_query),
                "search_intent": result.get("search_intent", "broad"),
                "key_concepts": result.get("key_concepts", []),
                "document_sections": result.get("document_sections", []),
                "response_style": result.get("response_style", "detailed"),
                "complexity_level": result.get("complexity_level", "medium"),
                "is_multi_intent": result.get("is_multi_intent", False),
                "sub_queries": result.get("sub_queries", [])
            }
            
            print(f"ðŸ”„ Query preprocessing: {original_query[:50]}... â†’ {processed_query['optimized_query']}")
            return processed_query
            
        except Exception as e:
            print(f"Query preprocessing error: {e}")
            # Enhanced fallback with simple keyword extraction
            return self._simple_query_preprocessing(original_query)

    def _simple_query_preprocessing(self, query: str) -> Dict[str, str]:
        """Fallback preprocessing when LLM fails"""
        query_lower = query.lower()
        
        # Simple keyword extraction
        words = re.findall(r'\b\w{3,}\b', query_lower)
        stop_words = {'iÃ§in', 'olan', 'ile', 'bir', 'bu', 'ÅŸu', 'da', 'de', 'ki', 've', 'veya', 
                     'bul', 'sÃ¶yle', 'anlat', 'ver', 'yap', 'bana', 'sana', 'Ã¼zerinden'}
        keywords = [w for w in words if w not in stop_words]
        
        # Determine intent based on keywords
        if any(word in query_lower for word in ['kimler', 'isim', 'ad', 'Ã§alÄ±ÅŸan']):
            search_intent = "specific"
            key_concepts = ["isim", "araÅŸtÄ±rmacÄ±", "danÄ±ÅŸman"]
        elif any(word in query_lower for word in ['bÃ¼tÃ§e', 'maliyet', 'para']):
            search_intent = "specific"
            key_concepts = ["bÃ¼tÃ§e", "maliyet", "kalem"]
        elif any(word in query_lower for word in ['zorluk', 'deÄŸerlendir', 'puan']):
            search_intent = "analytical"
            key_concepts = ["zorluk", "risk", "deÄŸerlendirme"]
        elif any(word in query_lower for word in ['amaÃ§', 'hedef', 'niÃ§in']):
            search_intent = "specific"
            key_concepts = ["amaÃ§", "hedef"]
        else:
            search_intent = "broad"
            key_concepts = keywords[:5]
        
        # Optimized query
        optimized_query = " ".join(key_concepts[:4])
        
        return {
            "original_query": query,
            "optimized_query": optimized_query,
            "search_intent": search_intent,
            "key_concepts": key_concepts,
            "document_sections": [],
            "response_style": "detailed",
            "complexity_level": "medium",
            "is_multi_intent": False,
            "sub_queries": []
        }

    def embed_sections_with_enhanced_chunking(self, data: Dict) -> List[Dict]:
        """Enhanced chunking with better context preservation - Table support enhanced"""
        chunks = []
        
        # Process all content types
        for section_name, content in data.items():
            if isinstance(content, str) and content.strip():
                # Split long sections into manageable chunks while preserving context
                text_chunks = self._smart_text_split(content.strip(), section_name)
                for i, chunk_text in enumerate(text_chunks):
                    chunks.append({
                        "section": f"{section_name}" + (f" (BÃ¶lÃ¼m {i+1})" if len(text_chunks) > 1 else ""),
                        "text": chunk_text,
                        "type": "text",
                        "priority": self._calculate_priority(section_name),
                        "keywords": self._extract_keywords(chunk_text)
                    })
            
            elif isinstance(content, dict):
                # Handle nested dictionaries
                for subkey, subval in content.items():
                    if isinstance(subval, str) and subval.strip():
                        chunks.append({
                            "section": f"{section_name} > {subkey}",
                            "text": subval.strip(),
                            "type": "subsection",
                            "priority": self._calculate_priority(f"{section_name} {subkey}"),
                            "keywords": self._extract_keywords(subval.strip())
                        })
                    elif isinstance(subval, dict):
                        # Handle Q&A pairs
                        for nested_key, nested_val in subval.items():
                            if isinstance(nested_val, str) and nested_val.strip():
                                chunks.append({
                                    "section": f"{section_name} > {subkey} > {nested_key}",
                                    "text": f"Soru: {nested_key}\nCevap: {nested_val}",
                                    "type": "qa_pair",
                                    "priority": 5,
                                    "keywords": self._extract_keywords(f"{nested_key} {nested_val}")
                                })
                    elif isinstance(subval, list):
                        # Handle lists within dict (like BÃ¼tÃ§e Kalemleri)
                        table_text = self._format_table(subval)
                        if table_text:
                            chunks.append({
                                "section": f"{section_name} > {subkey}",
                                "text": table_text,
                                "type": "table",
                                "priority": self._calculate_priority(f"{section_name} {subkey}"),
                                "keywords": self._extract_keywords(table_text)
                            })
            
            elif isinstance(content, list) and content:
                table_text = self._format_table(content)
                if table_text:
                    chunks.append({
                        "section": f"{section_name} (Tablo)",
                        "text": table_text,
                        "type": "table",
                        "priority": self._calculate_priority(section_name),
                        "keywords": self._extract_keywords(table_text)
                    })

        # Vectorize if model is available
        self.load_model()  # Force model loading
        if self.model and chunks:
            self._add_embeddings(chunks)
        else:
            print("âš ï¸  Proceeding without embeddings - using keyword search only")
        
        # DEBUG: Print chunks being created
        print(f"\nðŸ”¨ CHUNKING DEBUG:")
        print(f"ðŸ“¦ Created {len(chunks)} chunks:")
        for i, chunk in enumerate(chunks, 1):
            content_preview = chunk["text"][:100].replace('\n', ' ')
            print(f"  {i}. {chunk['section']} ({chunk['type']}) â†’ {content_preview}...")
        print()
        
        return chunks

    def _smart_text_split(self, text: str, section_name: str, max_chunk_size: int = 1000) -> List[str]:
        """Smart text splitting that preserves meaning"""
        if len(text) <= max_chunk_size:
            return [text]
        
        # Try to split by sentences first
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text]

    def _extract_keywords(self, text: str) -> List[str]:
        """Extract important keywords from text"""
        # Simple keyword extraction
        words = re.findall(r'\b\w{3,}\b', text.lower())
        # Filter out common Turkish stop words
        stop_words = {'iÃ§in', 'olan', 'olan', 'ile', 'bir', 'bu', 'ÅŸu', 'da', 'de', 'ki', 've', 'veya'}
        keywords = [w for w in words if w not in stop_words]
        return list(set(keywords))

    def _calculate_priority(self, section_name: str) -> int:
        """Calculate section priority based on content type - Rubric-enhanced"""
        section_lower = section_name.lower()
        
        # Rubric-based priority mapping
        rubric_priority_map = {
            'genel_bilgiler': 10,    # En Ã¶nemli - isimler burada
            'Ã¶zet': 9,               # Ã‡ok Ã¶nemli
            'amaÃ§_hedef': 9,         # Ã‡ok Ã¶nemli  
            'bÃ¼tÃ§e': 9,              # Ã‡ok Ã¶nemli (artÄ±rÄ±ldÄ±)
            'Ã¶nem_deÄŸer': 8,         # Ã–nemli
            'yaygÄ±n_etki': 7,        # Ã–nemli
            'yÃ¶ntem': 7,             # Ã–nemli
            'araÅŸtÄ±rma_olanaklarÄ±': 6, # Orta
            'iÅŸ_zaman': 6,           # Orta
            'risk': 5                # DÃ¼ÅŸÃ¼k
        }
        
        # Section name'e gÃ¶re kategori tespiti
        max_priority = 1
        
        # Ã–zel section kontrolÃ¼
        if 'genel' in section_lower and 'bilgi' in section_lower:
            max_priority = 10
        elif 'danÄ±ÅŸman' in section_lower:
            max_priority = 10  # DanÄ±ÅŸman chunk'larÄ± Ã§ok Ã¶nemli
        elif 'baÅŸvuru' in section_lower and 'sahibi' in section_lower:
            max_priority = 10  # BaÅŸvuru sahibi chunk'larÄ± Ã§ok Ã¶nemli
        elif 'bÃ¼tÃ§e' in section_lower:
            max_priority = 9   # BÃ¼tÃ§e chunk'larÄ± Ã§ok Ã¶nemli
        elif 'Ã¶zet' in section_lower:
            max_priority = 9
        elif 'amaÃ§' in section_lower or 'hedef' in section_lower:
            max_priority = 9
        elif 'yÃ¶ntem' in section_lower:
            max_priority = 7
        elif 'risk' in section_lower:
            max_priority = 5
        elif 'etki' in section_lower:
            max_priority = 7
        
        # Rubric keyword matching
        for category, rubric_info in self.rubric_section_map.items():
            keywords = rubric_info['keywords']
            if any(keyword in section_lower for keyword in keywords):
                candidate_priority = rubric_info['priority']
                max_priority = max(max_priority, candidate_priority)
        
        # Legacy keyword matching (fallback)
        for category, priority in rubric_priority_map.items():
            if any(keyword in section_lower for keyword in self.section_keywords.get(category, [])):
                max_priority = max(max_priority, priority)
        
        return max_priority

    def _add_embeddings(self, chunks: List[Dict]):
        """Add embeddings to chunks"""
        if not self.model:
            self.load_model()
            
        if not self.model:
            print("âš ï¸  No embedding model available, skipping vectorization")
            return
            
        try:
            print(f"ðŸ”„ Creating embeddings for {len(chunks)} chunks...")
            texts = [chunk["text"] for chunk in chunks]
            vectors = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
            
            for i, chunk in enumerate(chunks):
                chunk["vector"] = vectors[i]
                
            print(f"âœ… Embeddings created successfully!")
            
        except Exception as e:
            print(f"âŒ Embedding error: {e}")
            print("ðŸ”„ Continuing without embeddings...")

    def _format_table(self, table_data: List) -> str:
        """Format table data into readable text"""
        try:
            if not table_data:
                return ""
            
            formatted_rows = []
            for row in table_data:
                if isinstance(row, dict):
                    row_text = " | ".join([f"{k}: {v}" for k, v in row.items() if v])
                elif isinstance(row, list):
                    row_text = " | ".join([str(item) for item in row if item])
                else:
                    row_text = str(row)
                
                if row_text.strip():
                    formatted_rows.append(row_text)
            
            return "\n".join(formatted_rows)
        except:
            return json.dumps(table_data, ensure_ascii=False, indent=2)

    def enhanced_retrieval_with_preprocessing(self, processed_query: Dict, chunks: List[Dict]) -> List[Dict]:
        """
        Preprocessing edilmiÅŸ sorguya gÃ¶re geliÅŸmiÅŸ retrieval - Multi-intent destekli
        """
        # Multi-intent query kontrolÃ¼
        if processed_query.get("is_multi_intent", False) and processed_query.get("sub_queries"):
            return self._multi_intent_retrieval(processed_query, chunks)
        else:
            return self._single_intent_retrieval(processed_query, chunks)

    def _multi_intent_retrieval(self, processed_query: Dict, chunks: List[Dict]) -> List[Dict]:
        """
        Birden fazla intent iÃ§in AYRI AYRI embedding + benzerlik arama + birleÅŸtirme - DEBUG ENHANCED
        """
        all_relevant_chunks = {}
        sub_queries = processed_query.get("sub_queries", [])
        
        print(f"ðŸ” Multi-intent detected: {len(sub_queries)} sub-queries")
        
        # DEBUG: TÃ¼m chunk'larÄ± incele
        print(f"\nðŸ“‹ DEBUG: Available chunks:")
        for i, chunk in enumerate(chunks):
            content_preview = chunk["text"][:100].replace('\n', ' ')
            print(f"  {i+1}. {chunk['section']} â†’ {content_preview}...")
        
        for i, sub_query in enumerate(sub_queries):
            intent = sub_query.get("intent", "")
            keywords = sub_query.get("keywords", [])
            
            print(f"\n  Sub-query {i+1}: {intent} â†’ {keywords}")
            
            # Her sub-query iÃ§in AYRI embedding ve benzerlik aramasÄ±
            sub_matches = self._separate_embedding_search(sub_query, chunks)
            
            # DEBUG: Sub-query sonuÃ§larÄ±
            print(f"    ðŸŽ¯ Sub-query '{intent}' results:")
            for j, match in enumerate(sub_matches[:3]):
                content_preview = match["text"][:150].replace('\n', ' ')
                print(f"      {j+1}. {match['section']} (score: {match.get('similarity_score', 0):.3f})")
                print(f"         Content: {content_preview}...")
            
            # Intent bilgisini chunk'lara ekle
            for match in sub_matches:
                chunk_id = match["section"]
                if chunk_id not in all_relevant_chunks:
                    match["intent_types"] = [intent]
                    match["intent_scores"] = {intent: match.get("similarity_score", 0)}
                    all_relevant_chunks[chunk_id] = match
                else:
                    # AynÄ± chunk birden fazla intent'e hizmet ediyor
                    if intent not in all_relevant_chunks[chunk_id]["intent_types"]:
                        all_relevant_chunks[chunk_id]["intent_types"].append(intent)
                    all_relevant_chunks[chunk_id]["intent_scores"][intent] = match.get("similarity_score", 0)
        
        # Multi-intent scoring: Her intent'ten gelen score'larÄ± topla
        for chunk in all_relevant_chunks.values():
            intent_scores = chunk.get("intent_scores", {})
            total_similarity = sum(intent_scores.values())
            intent_count = len(chunk.get("intent_types", []))
            
            # Final score hesaplama
            chunk["total_score"] = (
                total_similarity * 2 +  # Similarity aÄŸÄ±rlÄ±k
                intent_count * 1.5 +    # Multi-intent bonus
                chunk.get("priority", 1) * 0.5
            )
            
            if intent_count > 1:
                chunk["multi_intent_bonus"] = True
                print(f"    ðŸ”— Multi-intent chunk: {chunk['section']} â†’ {chunk['intent_types']}")
        
        # Sort by total score
        sorted_chunks = sorted(all_relevant_chunks.values(), key=lambda x: x.get("total_score", 0), reverse=True)
        
        # DEBUG: Final results
        print(f"\nðŸ“Š FINAL RESULTS:")
        for i, chunk in enumerate(sorted_chunks[:5]):
            intent_info = ", ".join(chunk.get("intent_types", []))
            content_preview = chunk["text"][:100].replace('\n', ' ')
            print(f"  {i+1}. {chunk['section']} (score: {chunk.get('total_score', 0):.3f}) [{intent_info}]")
            print(f"     Content: {content_preview}...")
        
        # Multi-intent iÃ§in daha fazla chunk dÃ¶ndÃ¼r
        max_chunks = min(len(sub_queries) * 4, 15)  # Her intent iÃ§in 4 chunk, max 15
        return sorted_chunks[:max_chunks]

    def _separate_embedding_search(self, sub_query: Dict, chunks: List[Dict]) -> List[Dict]:
        """
        Tek bir sub-query iÃ§in AYRI embedding ve benzerlik aramasÄ±
        """
        intent = sub_query.get("intent", "")
        keywords = sub_query.get("keywords", [])
        
        # Intent'e gÃ¶re Ã¶zelleÅŸtirilmiÅŸ query oluÅŸtur
        optimized_query = self._create_intent_specific_query(intent, keywords)
        print(f"    ðŸŽ¯ Intent-specific query: '{optimized_query}'")
        
        matches = []
        
        # Emrecan modeli ile embedding ve benzerlik aramasÄ±
        if self.model and chunks and "vector" in chunks[0]:
            try:
                # Sub-query iÃ§in ayrÄ± embedding
                q_vec = self.model.encode([optimized_query], convert_to_numpy=True)[0]
                
                # Her chunk ile benzerlik hesapla
                for chunk in chunks:
                    if "vector" in chunk:
                        similarity = cosine_similarity([q_vec], [chunk["vector"]])[0][0]
                        
                        # Intent'e gÃ¶re threshold
                        threshold = self._get_intent_threshold(intent)
                        
                        if similarity > threshold:
                            chunk_copy = chunk.copy()
                            chunk_copy["similarity_score"] = similarity
                            chunk_copy["intent"] = intent
                            chunk_copy["optimized_query"] = optimized_query
                            matches.append(chunk_copy)
                
                print(f"    ðŸ“Š Found {len(matches)} matches for '{intent}' (threshold: {threshold})")
                
            except Exception as e:
                print(f"    âŒ Embedding error for '{intent}': {e}")
                # Fallback: keyword-based search
                matches = self._fallback_keyword_search(intent, keywords, chunks)
        else:
            # Model yoksa keyword-based search
            matches = self._fallback_keyword_search(intent, keywords, chunks)
        
        # Sort by similarity
        matches.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
        
        # Her intent iÃ§in top N chunk
        top_n = 5
        return matches[:top_n]

    def _create_intent_specific_query(self, intent: str, keywords: List[str]) -> str:
        """
        Intent'e gÃ¶re Ã¶zelleÅŸtirilmiÅŸ embedding query oluÅŸtur - Enhanced for person queries
        """
        base_keywords = " ".join(keywords)
        
        # Intent'e gÃ¶re ek kelimeler ekle
        if intent == "bÃ¼tÃ§e":
            return f"{base_keywords} bÃ¼tÃ§e maliyet toplam harcama kalem TL para Ã§izelge talep sarf malzeme demirbaÅŸ hizmet"
        
        elif intent in ["ekip", "ekip Ã¼yeleri", "genel_bilgiler_kiÅŸiler", "kiÅŸiler", "gÃ¶rev alan kiÅŸiler", "Ã§alÄ±ÅŸanlar", "Ã§alÄ±ÅŸan"]:
            return f"{base_keywords} baÅŸvuru sahibi danÄ±ÅŸman araÅŸtÄ±rmacÄ± isim ad kurum Ã§alÄ±ÅŸan personel ekip genel bilgiler"
        
        elif intent in ["danÄ±ÅŸman", "danÄ±ÅŸmanÄ±"]:
            return f"{base_keywords} danÄ±ÅŸman hoca Ã¶ÄŸretim Ã¼yesi doÃ§ent profesÃ¶r"
        
        elif intent == "aÃ§Ä±klama":
            return f"{base_keywords} aÃ§Ä±klama Ã¶zet amaÃ§ hedef proje tasarÄ±m"
        
        elif intent in ["zorluk", "deÄŸerlendirme", "derece"]:
            return f"{base_keywords} zorluk risk problem gÃ¼Ã§lÃ¼k karmaÅŸÄ±klÄ±k puan derece deÄŸerlendirme"
        
        elif intent == "yÃ¶ntem":
            return f"{base_keywords} yÃ¶ntem teknik sÃ¼reÃ§ araÅŸtÄ±rma metod tasarÄ±m"
        
        elif intent == "zaman":
            return f"{base_keywords} zaman takvim sÃ¼re Ã§izelge iÅŸ paketi program"
        
        else:
            return base_keywords

    def _get_intent_threshold(self, intent: str) -> float:
        """
        Intent'e gÃ¶re benzerlik threshold'u belirle - Q&A format iÃ§in optimize
        """
        # Q&A formatÄ±nda kritik intent'ler iÃ§in Ã§ok dÃ¼ÅŸÃ¼k threshold
        if intent in ["bÃ¼tÃ§e", "ekip", "kiÅŸiler", "genel_bilgiler_kiÅŸiler"]:
            return 0.08  # Ã‡ok dÃ¼ÅŸÃ¼k - mutlaka bulabilmesi iÃ§in
        
        elif intent == "aÃ§Ä±klama":
            return 0.12  # DÃ¼ÅŸÃ¼k seviye
        
        else:
            return 0.20   # Normal threshold

    def _fallback_keyword_search(self, intent: str, keywords: List[str], chunks: List[Dict]) -> List[Dict]:
        """
        Model Ã§alÄ±ÅŸmazsa keyword-based fallback search - Enhanced for person queries
        """
        print(f"    ðŸ”„ Fallback keyword search for '{intent}'")
        
        matches = []
        
        # Intent'e gÃ¶re Ã¶zel kelimeler ve section kelimeleri - Enhanced person detection
        if intent == "bÃ¼tÃ§e":
            search_words = [
                "bÃ¼tÃ§e", "maliyet", "tl", "para", "harcama", "kalem", "toplam", "talep", "Ã§izelge", 
                "sarf", "malzeme", "demirbaÅŸ", "hizmet", "fiyat", "tutar"
            ] + keywords
            section_words = ["bÃ¼tÃ§e", "talep", "Ã§izelge", "maliyet"]
            
        elif intent in ["ekip", "kiÅŸiler", "genel_bilgiler_kiÅŸiler", "ekip Ã¼yeleri", "gÃ¶rev alan kiÅŸiler", "Ã§alÄ±ÅŸanlar", "Ã§alÄ±ÅŸan"]:
            search_words = [
                "baÅŸvuru", "sahibi", "danÄ±ÅŸman", "araÅŸtÄ±rmacÄ±", "isim", "ad", "kurum", 
                "personel", "ekip", "Ã§alÄ±ÅŸan", "yÃ¼rÃ¼tÃ¼cÃ¼", "sorumlu", "bakar", "baykaran"
            ] + keywords
            section_words = ["genel", "bilgi", "baÅŸvuru", "ekip", "sahibi"]  # "sahibi" eklendi
            
        elif intent in ["danÄ±ÅŸman", "danÄ±ÅŸmanÄ±"]:
            search_words = [
                "danÄ±ÅŸman", "hoca", "Ã¶ÄŸretim", "Ã¼yesi", "doÃ§ent", "profesÃ¶r", "dr", "coÅŸkun", "selÃ§uk"
            ] + keywords
            section_words = ["genel", "bilgi", "danÄ±ÅŸman"]  # Ã–zel danÄ±ÅŸman section'Ä±
            
        elif intent == "aÃ§Ä±klama":
            search_words = [
                "aÃ§Ä±klama", "Ã¶zet", "proje", "tasarÄ±m", "amaÃ§", "hedef", "Ã¶nerisi"
            ] + keywords
            section_words = ["Ã¶zet", "aÃ§Ä±klama", "Ã¶zgÃ¼n", "amaÃ§"]
            
        else:
            search_words = keywords
            section_words = []
        
        print(f"      ðŸ” Searching for: {search_words[:12]}...")
        
        for chunk in chunks:
            text_lower = chunk["text"].lower()
            section_lower = chunk["section"].lower()
            
            score = 0
            matched_words = []
            
            # Section name matching (Ã§ok yÃ¼ksek puan)
            for section_word in section_words:
                if section_word in section_lower:
                    # Ã–zel bonuslar
                    if intent in ["danÄ±ÅŸman", "danÄ±ÅŸmanÄ±"] and "danÄ±ÅŸman" in section_lower:
                        score += 15  # Ã‡ok yÃ¼ksek bonus danÄ±ÅŸman iÃ§in
                    elif intent in ["Ã§alÄ±ÅŸanlar", "Ã§alÄ±ÅŸan"] and "baÅŸvuru" in section_lower:
                        score += 12  # YÃ¼ksek bonus baÅŸvuru sahibi iÃ§in
                    else:
                        score += 8
                    matched_words.append(f"section:{section_word}")
            
            # Text content matching - Enhanced for names
            for word in search_words:
                word_lower = word.lower()
                
                # Exact word matching
                if word_lower in text_lower:
                    # Ä°sim ve unvan kelimeleri iÃ§in Ã¶zel bonus
                    if word_lower in ["bakar", "baykaran", "semih", "ibrahim", "berkay", "selÃ§uk", "coÅŸkun", "dr"]:
                        score += 5  # Ä°sim bonusu
                    elif len(word_lower) >= 5:  # Uzun kelimeler daha deÄŸerli
                        score += 3
                    else:
                        score += 2
                    matched_words.append(f"text:{word}")
                
                # Section matching
                if word_lower in section_lower:
                    score += 2  # Section matching artÄ±rÄ±ldÄ±
                    matched_words.append(f"section:{word}")
            
            if score > 0:
                chunk_copy = chunk.copy()
                chunk_copy["similarity_score"] = min(score / 20, 1.0)  # Yeni normalize deÄŸeri
                chunk_copy["intent"] = intent
                chunk_copy["keyword_score"] = score
                chunk_copy["matched_words"] = matched_words
                matches.append(chunk_copy)
                
                # DEBUG: Match details
                content_preview = chunk['text'][:150].replace('\n', ' ')
                print(f"      âœ… Match: {chunk['section']} (score: {score})")
                print(f"         Matched: {', '.join(matched_words[:8])}...")
                print(f"         Content: {content_preview}...")
        
        print(f"      ðŸ“Š Total matches found: {len(matches)}")
        return matches

    def _retrieve_for_sub_query(self, sub_query: Dict, chunks: List[Dict]) -> List[Dict]:
        """
        Tek bir sub-query iÃ§in retrieval - Rubric-enhanced section matching
        """
        intent = sub_query.get("intent", "")
        keywords = sub_query.get("keywords", [])
        
        matches = []
        
        # 1. Rubric-based Section Matching (En Ã¶nemli)
        for chunk in chunks:
            section_lower = chunk["section"].lower()
            section_score = 0
            
            # Intent'e gÃ¶re rubric section mapping
            if intent in ["danÄ±ÅŸman", "danÄ±ÅŸmanÄ±"]:
                if "danÄ±ÅŸman" in section_lower or ("genel" in section_lower and "bilgi" in section_lower):
                    section_score += 20  # Ã‡ok yÃ¼ksek puan
            elif intent in ["Ã§alÄ±ÅŸanlar", "Ã§alÄ±ÅŸan", "kiÅŸiler"]:
                if "baÅŸvuru" in section_lower and "sahibi" in section_lower:
                    section_score += 18  # Ã‡ok yÃ¼ksek puan baÅŸvuru sahibi iÃ§in
                elif "genel" in section_lower and "bilgi" in section_lower:
                    section_score += 15  # YÃ¼ksek puan genel bilgiler iÃ§in
            elif intent == "bÃ¼tÃ§e":
                if "bÃ¼tÃ§e" in section_lower:
                    section_score += 20  # Ã‡ok yÃ¼ksek puan
            elif intent == "aÃ§Ä±klama":
                if "Ã¶zet" in section_lower:
                    section_score += 15
                elif "amaÃ§" in section_lower or "hedef" in section_lower:
                    section_score += 12
            
            if section_score > 0:
                chunk_copy = chunk.copy()
                chunk_copy["section_score"] = section_score
                matches.append(chunk_copy)
        
        # 2. Enhanced Keyword Matching
        for chunk in chunks:
            text_lower = chunk["text"].lower()
            section_lower = chunk["section"].lower()
            keyword_score = 0
            
            # Intent-specific keyword scoring with rubric knowledge
            if intent in ["danÄ±ÅŸman", "danÄ±ÅŸmanÄ±"]:
                danÄ±ÅŸman_keywords = ["danÄ±ÅŸman", "hoca", "Ã¶ÄŸretim", "Ã¼yesi", "dr", "doÃ§", "prof", "selÃ§uk", "coÅŸkun"]
                for kw in danÄ±ÅŸman_keywords:
                    if kw in text_lower or kw in section_lower:
                        keyword_score += 4
                        
            elif intent in ["Ã§alÄ±ÅŸanlar", "Ã§alÄ±ÅŸan", "kiÅŸiler"]:
                kiÅŸi_keywords = ["baÅŸvuru", "sahibi", "semih", "bakar", "ibrahim", "berkay", "araÅŸtÄ±rmacÄ±"]
                for kw in kiÅŸi_keywords:
                    if kw in text_lower or kw in section_lower:
                        keyword_score += 4
                        
            elif intent == "bÃ¼tÃ§e":
                bÃ¼tÃ§e_keywords = ["bÃ¼tÃ§e", "talep", "kalem", "maliyet", "tl", "sarf", "malzeme"]
                for kw in bÃ¼tÃ§e_keywords:
                    if kw in text_lower or kw in section_lower:
                        keyword_score += 3
            
            # Original keyword matching
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    keyword_score += 2
                if keyword.lower() in section_lower:
                    keyword_score += 1
            
            if keyword_score > 0:
                existing_match = next((m for m in matches if m["section"] == chunk["section"]), None)
                if existing_match:
                    existing_match["keyword_score"] = existing_match.get("keyword_score", 0) + keyword_score
                else:
                    chunk_copy = chunk.copy()
                    chunk_copy["keyword_score"] = keyword_score
                    matches.append(chunk_copy)
        
        # 3. Semantic matching (if available)
        if self.model and chunks and "vector" in chunks[0]:
            try:
                # Enhanced query for semantic search
                enhanced_queries = {
                    "bÃ¼tÃ§e": "bÃ¼tÃ§e maliyet toplam harcama talep Ã§izelge",
                    "danÄ±ÅŸman": "danÄ±ÅŸman hoca Ã¶ÄŸretim Ã¼yesi dr",
                    "danÄ±ÅŸmanÄ±": "danÄ±ÅŸman hoca Ã¶ÄŸretim Ã¼yesi dr",
                    "Ã§alÄ±ÅŸanlar": "baÅŸvuru sahibi araÅŸtÄ±rmacÄ±",
                    "Ã§alÄ±ÅŸan": "baÅŸvuru sahibi araÅŸtÄ±rmacÄ±",
                    "kiÅŸiler": "baÅŸvuru sahibi araÅŸtÄ±rmacÄ±"
                }
                
                query_text = enhanced_queries.get(intent, " ".join(keywords))
                q_vec = self.model.encode([query_text], convert_to_numpy=True)[0]
                
                for chunk in chunks:
                    if "vector" in chunk:
                        sim = cosine_similarity([q_vec], [chunk["vector"]])[0][0]
                        if sim > 0.15:  # Lower threshold for better coverage
                            existing_match = next((m for m in matches if m["section"] == chunk["section"]), None)
                            if existing_match:
                                existing_match["semantic_score"] = sim
                            else:
                                chunk_copy = chunk.copy()
                                chunk_copy["semantic_score"] = sim
                                matches.append(chunk_copy)
            except Exception as e:
                print(f"Semantic matching error for sub-query: {e}")
        
        # Calculate total scores with rubric weighting
        for match in matches:
            total_score = (
                match.get("section_score", 0) * 2 +      # Section matching Ã§ok Ã¶nemli
                match.get("keyword_score", 0) * 1.5 +    # Keyword matching Ã¶nemli
                match.get("semantic_score", 0) * 3 +     # Semantic matching Ã¶nemli
                match.get("priority", 1) * 0.3           # Priority dÃ¼ÅŸÃ¼k aÄŸÄ±rlÄ±k
            )
            match["total_score"] = total_score
        
        # Return top matches for this sub-query
        matches.sort(key=lambda x: x.get("total_score", 0), reverse=True)
        return matches[:6]  # Her sub-query iÃ§in max 6 chunk (artÄ±rÄ±ldÄ±)

    def _single_intent_retrieval(self, processed_query: Dict, chunks: List[Dict]) -> List[Dict]:
        """
        Tek intent iÃ§in mevcut retrieval logic'i
        """
        optimized_query = processed_query["optimized_query"]
        search_intent = processed_query["search_intent"]
        key_concepts = processed_query["key_concepts"]
        document_sections = processed_query["document_sections"]
        
        # Intent'e gÃ¶re retrieval stratejisi
        if search_intent == "specific":
            top_k = 3
            focus_on_exact_match = True
        elif search_intent == "broad":
            top_k = 6
            focus_on_exact_match = False
        elif search_intent == "comparative":
            top_k = 8
            focus_on_exact_match = False
        else:  # analytical
            top_k = 10
            focus_on_exact_match = False
        
        # 1. Section-based retrieval (bÃ¶lÃ¼m odaklÄ±)
        section_matches = []
        if document_sections:
            for section in document_sections:
                section_chunks = [c for c in chunks if any(s.lower() in c["section"].lower() for s in document_sections)]
                section_matches.extend(section_chunks)
        
        # 2. Concept-based retrieval (kavram odaklÄ±)
        concept_matches = []
        for chunk in chunks:
            chunk_text_lower = chunk["text"].lower()
            concept_score = sum(1 for concept in key_concepts if concept.lower() in chunk_text_lower)
            if concept_score > 0:
                chunk_copy = chunk.copy()
                chunk_copy["concept_score"] = concept_score
                concept_matches.append(chunk_copy)
        
        # 3. Semantic retrieval (anlam odaklÄ±)
        semantic_matches = []
        if self.model and chunks and "vector" in chunks[0]:
            try:
                q_vec = self.model.encode([optimized_query], convert_to_numpy=True)[0]
                for chunk in chunks:
                    if "vector" in chunk:
                        sim = cosine_similarity([q_vec], [chunk["vector"]])[0][0]
                        chunk_copy = chunk.copy()
                        chunk_copy["semantic_score"] = sim
                        semantic_matches.append(chunk_copy)
            except Exception as e:
                print(f"Semantic matching error: {e}")
        
        # 4. Combine and rank all matches
        all_matches = {}
        
        # Section matches (yÃ¼ksek aÄŸÄ±rlÄ±k)
        for match in section_matches:
            chunk_id = match["section"]
            if chunk_id not in all_matches:
                all_matches[chunk_id] = match.copy()
                all_matches[chunk_id]["total_score"] = match.get("priority", 1) * 3
            else:
                all_matches[chunk_id]["total_score"] += match.get("priority", 1) * 3
        
        # Concept matches (orta aÄŸÄ±rlÄ±k)
        for match in concept_matches:
            chunk_id = match["section"]
            concept_bonus = match.get("concept_score", 0) * 2
            if chunk_id not in all_matches:
                all_matches[chunk_id] = match.copy()
                all_matches[chunk_id]["total_score"] = concept_bonus
            else:
                all_matches[chunk_id]["total_score"] += concept_bonus
        
        # Semantic matches (dÃ¼ÅŸÃ¼k aÄŸÄ±rlÄ±k)
        for match in semantic_matches:
            chunk_id = match["section"]
            semantic_bonus = match.get("semantic_score", 0) * 1.5
            if chunk_id not in all_matches:
                all_matches[chunk_id] = match.copy()
                all_matches[chunk_id]["total_score"] = semantic_bonus
            else:
                all_matches[chunk_id]["total_score"] += semantic_bonus
        
        # Sort by total score
        sorted_matches = sorted(all_matches.values(), key=lambda x: x.get("total_score", 0), reverse=True)
        
        return sorted_matches[:top_k]

    def generate_contextual_answer(self, processed_query: Dict, context_chunks: List[Dict], model: str = "gpt-4o-mini") -> str:
        """
        TEK LLM Ä°LE EN KALÄ°TELÄ° YANITLAR - Enhanced Context Formatting
        """
        original_query = processed_query["original_query"]
        response_style = processed_query["response_style"]
        complexity_level = processed_query["complexity_level"]
        is_multi_intent = processed_query.get("is_multi_intent", False)
        sub_queries = processed_query.get("sub_queries", [])
        
        if not context_chunks:
            return "ÃœzgÃ¼nÃ¼m, sorunuzla ilgili dokÃ¼man iÃ§eriÄŸinde bilgi bulunamadÄ±."
        
        # ENHANCED CONTEXT FORMATTING - Her intent iÃ§in ayrÄ± organize
        if is_multi_intent and sub_queries:
            combined_context = self._create_intent_focused_context(context_chunks, sub_queries)
        else:
            combined_context = self._create_standard_context(context_chunks)
        
        # Enhanced Multi-Intent Prompt
        if is_multi_intent:
            intent_instructions = self._create_intent_instructions(sub_queries)
            multi_intent_instruction = f"""
ðŸ“‹ Ã–NEMLÄ°: Bu soruda {len(sub_queries)} farklÄ± konu var:
{intent_instructions}

ðŸŽ¯ YANIT YAPISI:
â€¢ Her konuyu AYRI BAÅžLIK altÄ±nda ele al
â€¢ Her konu iÃ§in ilgili bÃ¶lÃ¼mlerden bilgi ver
â€¢ Bilgi bulunamazsa aÃ§Ä±kÃ§a belirt
â€¢ Konular: {', '.join([sq.get('intent', '') for sq in sub_queries])}"""
        else:
            multi_intent_instruction = ""
        
        # Response style instructions
        style_instructions = {
            "detailed": "DetaylÄ± ve kapsamlÄ± bir aÃ§Ä±klama yap. TÃ¼m Ã¶nemli noktalarÄ± ele al.",
            "summary": "KÄ±sa ve Ã¶z bir Ã¶zet ver. Ana noktalarÄ± vurgula.",
            "list": "Madde madde listele. Net ve dÃ¼zenli bir format kullan.",
            "analytical": "Analitik bir yaklaÅŸÄ±mla deÄŸerlendir. GÃ¼Ã§lÃ¼ ve zayÄ±f yÃ¶nleri belirt."
        }
        
        style_instruction = style_instructions.get(response_style, "Net ve anlaÅŸÄ±lÄ±r ÅŸekilde yanÄ±tla.")
        
        # ENHANCED PROMPT - Her detayÄ± aÃ§Ä±k
        prompt = f"""Sen TÃœBÄ°TAK 2209-A projelerini analiz eden uzmansÄ±n. AÅŸaÄŸÄ±daki dokÃ¼man bÃ¶lÃ¼mlerini kullanarak kullanÄ±cÄ±nÄ±n sorusunu EKSIKSIZ yanÄ±tla.

ðŸ“‹ PROJE DOKÃœMANI:
{combined_context}

â“ KULLANICI SORUSU: {original_query}

ðŸ“ YANIT TALÄ°MATLARI:
â€¢ {style_instruction}{multi_intent_instruction}
â€¢ MUTLAKA hangi bÃ¶lÃ¼mden aldÄ±ÄŸÄ±n bilgiyi belirt (Ã¶rn: "Genel Bilgiler bÃ¶lÃ¼mÃ¼nde...")
â€¢ EÄŸer bir konu hakkÄ±nda bilgi bulunamÄ±yorsa, bunu aÃ§Ä±kÃ§a belirt
â€¢ TÃ¼rkÃ§e, profesyonel bir dil kullan
â€¢ DokÃ¼man dÄ±ÅŸÄ±nda bilgi EKLEME

ðŸ” KONTROL LÄ°STESÄ°:
â€¢ TÃ¼m sorulan konularÄ± yanÄ±tladÄ±n mÄ±?
â€¢ Her bilgi iÃ§in kaynak belirttiniz mi?
â€¢ Eksik bilgiler iÃ§in aÃ§Ä±klama yaptÄ±nÄ±z mÄ±?

ðŸ’¡ YANIT:"""

        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system", 
                        "content": "Sen TÃœBÄ°TAK proje deÄŸerlendirme uzmanÄ±sÄ±n. Verilen dokÃ¼mandan EKSIKSIZ bilgi Ã§Ä±karÄ±rsÄ±n. Her soruyu mutlaka yanÄ±tlarsÄ±n, bilgi yoksa bunu belirtirsin."
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2500,  # Daha uzun yanÄ±tlar iÃ§in
                temperature=0.2   # Daha tutarlÄ± sonuÃ§lar
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"OpenAI API hatasÄ±: {str(e)}")
            return "ÃœzgÃ¼nÃ¼m, ÅŸu anda yanÄ±t oluÅŸturma servisi kullanÄ±lamÄ±yor. LÃ¼tfen daha sonra tekrar deneyin."

    def _create_intent_focused_context(self, context_chunks: List[Dict], sub_queries: List[Dict]) -> str:
        """
        Her intent iÃ§in odaklanmÄ±ÅŸ context oluÅŸtur
        """
        context_parts = []
        
        # Intent'lere gÃ¶re chunk'larÄ± organize et
        intent_chunks = {}
        
        for chunk in context_chunks:
            intent_types = chunk.get("intent_types", ["genel"])
            for intent in intent_types:
                if intent not in intent_chunks:
                    intent_chunks[intent] = []
                intent_chunks[intent].append(chunk)
        
        # Her intent iÃ§in organize bÃ¶lÃ¼m
        for i, sub_query in enumerate(sub_queries):
            intent = sub_query.get("intent", "")
            intent_display = intent.replace("_", " ").title()
            
            context_parts.append(f"\n{'='*50}")
            context_parts.append(f"ðŸ“Œ {intent_display.upper()} Ä°LE Ä°LGÄ°LÄ° BÄ°LGÄ°LER:")
            context_parts.append('='*50)
            
            if intent in intent_chunks:
                for j, chunk in enumerate(intent_chunks[intent]):
                    context_parts.append(f"\nBÃ–LÃœM {i+1}.{j+1}: {chunk['section']}")
                    context_parts.append("-" * 40)
                    context_parts.append(chunk['text'])
                    context_parts.append("-" * 40)
            else:
                context_parts.append(f"\nâš ï¸ {intent_display} ile ilgili dokÃ¼man bÃ¶lÃ¼mÃ¼ bulunamadÄ±.")
        
        # Genel bÃ¶lÃ¼mler (intent'e atanmayan)
        general_chunks = [c for c in context_chunks if "genel" in c.get("intent_types", [])]
        if general_chunks:
            context_parts.append(f"\n{'='*50}")
            context_parts.append("ðŸ“Œ GENEL BÄ°LGÄ°LER:")
            context_parts.append('='*50)
            
            for j, chunk in enumerate(general_chunks):
                context_parts.append(f"\nGENEL BÃ–LÃœM {j+1}: {chunk['section']}")
                context_parts.append("-" * 40)
                context_parts.append(chunk['text'])
                context_parts.append("-" * 40)
        
        return "\n".join(context_parts)

    def _create_standard_context(self, context_chunks: List[Dict]) -> str:
        """
        Standart context formatÄ±
        """
        context_parts = []
        for i, chunk in enumerate(context_chunks, 1):
            context_parts.append(f"BÃ–LÃœM {i}: {chunk['section']}")
            context_parts.append("-" * 40)
            context_parts.append(chunk['text'])
            context_parts.append("-" * 40)
        
        return "\n".join(context_parts)

    def _create_intent_instructions(self, sub_queries: List[Dict]) -> str:
        """
        Her intent iÃ§in talimat oluÅŸtur
        """
        instructions = []
        for i, sub_query in enumerate(sub_queries, 1):
            intent = sub_query.get("intent", "")
            keywords = sub_query.get("keywords", [])
            intent_display = intent.replace("_", " ").title()
            
            instructions.append(f"{i}. {intent_display}: {', '.join(keywords)}")
        
        return "\n".join(instructions)

    def _format_multi_intent_context(self, context_chunks: List[Dict], processed_query: Dict) -> List[str]:
        """
        Multi-intent query'ler iÃ§in context'i organize et - DEBUG ENHANCED
        """
        context_parts = []
        sub_queries = processed_query.get("sub_queries", [])
        
        print(f"\nðŸŽ¨ FORMATTING CONTEXT for {len(context_chunks)} chunks:")
        
        # Intent'lere gÃ¶re chunk'larÄ± grupla
        intent_chunks = {}
        for chunk in context_chunks:
            intent_types = chunk.get("intent_types", ["genel"])
            
            # DEBUG: Chunk intent mapping
            content_preview = chunk["text"][:80].replace('\n', ' ')
            print(f"  ðŸ“ {chunk['section']} â†’ intents: {intent_types}")
            print(f"     Content: {content_preview}...")
            
            for intent in intent_types:
                if intent not in intent_chunks:
                    intent_chunks[intent] = []
                intent_chunks[intent].append(chunk)
        
        # Her intent iÃ§in ayrÄ± bÃ¶lÃ¼m oluÅŸtur
        section_counter = 1
        for intent, chunks in intent_chunks.items():
            context_parts.append(f"ðŸ“Œ {intent.upper()} Ä°LE Ä°LGÄ°LÄ° BÃ–LÃœMLER:")
            
            for chunk in chunks:
                multi_intent_info = "ðŸ”— Ã‡OKLU KONU" if chunk.get("multi_intent_bonus", False) else ""
                intent_scores = chunk.get("intent_scores", {})
                score_info = f" (scores: {intent_scores})" if intent_scores else ""
                
                context_parts.append(f"""BÃ–LÃœM {section_counter}: {chunk['section']} {multi_intent_info}{score_info}
{chunk['text']}
---""")
                section_counter += 1
                
                # DEBUG: Context content
                print(f"    âœ… Added to context: {chunk['section']}")
                print(f"       Intent scores: {intent_scores}")
            
            context_parts.append("")  # BoÅŸ satÄ±r
        
        print(f"ðŸŽ¨ Total context parts: {len(context_parts)}")
        return context_parts

    # MAIN ENTRY POINT - Yeni ana method
    def process_query_with_preprocessing(self, user_query: str, chunks: List[Dict], model: str = "gpt-4o-mini") -> Tuple[str, Dict]:
        """
        Ana entry point: Query preprocessing + Enhanced retrieval + Contextual generation
        """
        # 1. Preprocessing
        processed_query = self.preprocess_user_query(user_query)
        
        # 2. Enhanced retrieval
        relevant_chunks = self.enhanced_retrieval_with_preprocessing(processed_query, chunks)
        
        # 3. Generate answer
        answer = self.generate_contextual_answer(processed_query, relevant_chunks, model)
        
        # 4. Return answer and metadata
        metadata = {
            "original_query": user_query,
            "optimized_query": processed_query["optimized_query"],
            "search_intent": processed_query["search_intent"],
            "found_chunks": len(relevant_chunks),
            "response_style": processed_query["response_style"],
            "best_section": relevant_chunks[0]["section"] if relevant_chunks else "N/A",
            "relevance_score": relevant_chunks[0].get("total_score", 0) if relevant_chunks else 0,
            "is_multi_intent": processed_query.get("is_multi_intent", False),
            "sub_queries_count": len(processed_query.get("sub_queries", [])),
            "intent_coverage": self._calculate_intent_coverage(processed_query, relevant_chunks)
        }
        
        return answer, metadata

    def _calculate_intent_coverage(self, processed_query: Dict, relevant_chunks: List[Dict]) -> Dict:
        """
        Multi-intent query'de kaÃ§ intent'in coverage aldÄ±ÄŸÄ±nÄ± hesapla
        """
        if not processed_query.get("is_multi_intent", False):
            return {"single_intent": True}
        
        sub_queries = processed_query.get("sub_queries", [])
        if not sub_queries:
            return {"coverage": "unknown"}
        
        # Her intent iÃ§in chunk bulundu mu kontrol et
        intent_coverage = {}
        for sub_query in sub_queries:
            intent = sub_query.get("intent", "")
            has_chunks = any(intent in chunk.get("intent_types", []) for chunk in relevant_chunks)
            intent_coverage[intent] = has_chunks
        
        covered_intents = sum(1 for covered in intent_coverage.values() if covered)
        total_intents = len(sub_queries)
        
        return {
            "total_intents": total_intents,
            "covered_intents": covered_intents,
            "coverage_percentage": round((covered_intents / total_intents) * 100, 1) if total_intents > 0 else 0,
            "intent_details": intent_coverage
        }

    # Backward compatibility methods - Eski route iÃ§in optimize edilmiÅŸ
    def embed_sections_with_local_model(self, data: Dict) -> List[Dict]:
        """Backward compatibility wrapper - Eski route'tan Ã§aÄŸrÄ±lÄ±r"""
        return self.embed_sections_with_enhanced_chunking(data)
    
    def find_best_matches_enhanced(self, question: str, embedded_chunks: List[Dict], top_k: int = 3) -> List[Dict]:
        """Backward compatibility wrapper - Multi-intent sistemini eski format'ta dÃ¶ndÃ¼rÃ¼r"""
        print(f"ðŸ”„ Using enhanced multi-intent system for: {question[:50]}...")
        
        # Yeni multi-intent sistemini kullan
        answer, metadata = self.process_query_with_preprocessing(question, embedded_chunks, "gpt-4o-mini")
        
        # Metadata'dan chunk bilgilerini Ã§Ä±kar
        found_chunks = metadata.get('found_chunks', 0)
        best_section = metadata.get('best_section', 'N/A')
        relevance_score = metadata.get('relevance_score', 0)
        
        # Eski format'ta sonuÃ§ dÃ¶ndÃ¼r (route'un beklediÄŸi format)
        formatted_matches = []
        
        # BaÅŸarÄ±lÄ± yanÄ±t varsa chunk bilgilerini simÃ¼le et
        if found_chunks > 0 and answer and "bilgi bulunamadÄ±" not in answer.lower():
            for i in range(min(top_k, found_chunks)):
                formatted_matches.append({
                    "section": best_section if i == 0 else f"Related Section {i+1}",
                    "text": answer[:500] if i == 0 else f"Additional context for {question}...",
                    "type": "enhanced_multi_intent",
                    "similarity": 0.8 if i == 0 else max(0.3, 0.8 - (i * 0.1)),  # Decreasing similarity
                    "combined_score": float(relevance_score) if i == 0 else max(0.2, float(relevance_score) - (i * 0.2)),
                    "keyword_match": metadata.get('sub_queries_count', 1),
                    "enhanced_answer": answer  # YanÄ±tÄ± chunk'a gÃ¶mme
                })
        else:
            # BaÅŸarÄ±sÄ±z durumda bile bir match dÃ¶ndÃ¼r ki route Ã§alÄ±ÅŸsÄ±n
            formatted_matches.append({
                "section": "No Match Found",
                "text": "DokÃ¼man analiz edildi ancak yeterli bilgi bulunamadÄ±.",
                "type": "no_match",
                "similarity": 0.1,
                "combined_score": 0.1,
                "keyword_match": 0,
                "enhanced_answer": answer
            })
        
        print(f"âœ… Returning {len(formatted_matches)} formatted matches to old route")
        return formatted_matches
    
    def find_best_matches(self, question: str, embedded_chunks: List[Dict], top_k: int = 3) -> List[Dict]:
        """Backward compatibility wrapper"""
        return self.find_best_matches_enhanced(question, embedded_chunks, top_k)
    
    def ask_openai(self, question: str, context_chunks: List[Dict], model: str = "gpt-4o-mini") -> str:
        """Backward compatibility wrapper - Eski route'un OpenAI Ã§aÄŸrÄ±sÄ±nÄ± yakalar"""
        
        # EÄŸer enhanced_answer varsa onu dÃ¶ndÃ¼r (zaten iÅŸlenmiÅŸ)
        if context_chunks and context_chunks[0].get("enhanced_answer"):
            enhanced_answer = context_chunks[0]["enhanced_answer"]
            print(f"âœ… Returning pre-processed enhanced answer")
            return enhanced_answer
        
        # Fallback: Normal iÅŸleme
        processed_query = self.preprocess_user_query(question)
        return self.generate_contextual_answer(processed_query, context_chunks, model)

# Global instance
rag_system = TurkishRAGSystem()

# Backward compatibility functions
def embed_sections_with_local_model(data):
    return rag_system.embed_sections_with_local_model(data)

def find_best_match(question, embedded_chunks):
    matches = rag_system.find_best_matches(question, embedded_chunks, top_k=1)
    if matches:
        return {
            "section": matches[0]["section"],
            "context": matches[0]["text"],
            "similarity": matches[0]["similarity"]
        }
    return {
        "section": "BulunamadÄ±",
        "context": "Ä°lgili bÃ¶lÃ¼m bulunamadÄ±",
        "similarity": 0.0
    }